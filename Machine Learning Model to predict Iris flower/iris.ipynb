{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import iris dataset from sklearn and then load it\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# There is no need to clean data as we assume that our imported data from sklearn is enough cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data # Input data\n",
    "y = iris.target # output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = iris.feature_names # Names of our input columns\n",
    "target_names = iris.target_names # Name of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "# Spliting dataset into training set and testing set\n",
    "from sklearn.model_selection import train_test_split # splits array and matrics into random train and test subsets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our Machine Learning Model\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3) # No of neighbors as we want to assume for our prediction\n",
    "\n",
    "knn.fit(X_train, y_train) # Model created\n",
    "\n",
    "y_pred = knn.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Checking our output with our created model\n",
    "\n",
    "from sklearn import metrics \n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions : ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Testing our model on the random data inputs to check the output\n",
    "\n",
    "sample = [[3,5,4,2], [2,3,5,4]] # This is our random sample which i have randomly selected from my funny mind\n",
    "\n",
    "# Using our model which one we have already created before, make our prediction\n",
    "\n",
    "predictions = knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in predictions]\n",
    "\n",
    "print('predictions :', pred_species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mybrain.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whenever, our data set will be bigger and bigger, it will take a hug time to train our model.\n",
    "# sometimes, it will be impossible for us to train our model in our regular computers.\n",
    "# For this type of case, if we re build our model again and again for every step of our work\n",
    "# This will consume a lot of times. So to solve this problem, we can save our model\n",
    "# By this time, scikit learn will again help us to save our model\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(knn, 'mybrain.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions : ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load('mybrain.joblib')\n",
    "model.predict(X_test)\n",
    "# Here we reuse our model not by re building our model just saving in joblibS\n",
    "sample = [[3,5,4,2], [2,3,5,4]]\n",
    "predictions = knn.predict(sample)\n",
    "pred_species = [iris.target_names[p] for p in predictions]\n",
    "\n",
    "print('predictions :', pred_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
